{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Super Resolution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1oNjpectrfVIYVDnTg7tH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanshenoy/image-super-resolution/blob/master/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny041tz9ST3g",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Tensorboard for visual logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc1vfLv5Q5m7",
        "colab_type": "code",
        "outputId": "d376dc3f-2248-439d-ac61-f2df8de88985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        }
      },
      "source": [
        "%load_ext tensorboard \n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 3486), started 2:23:39 ago. (Use '!kill 3486' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6rVbHelSbgo",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzq2QnWhRbBn",
        "colab_type": "code",
        "outputId": "4e7edc2f-b441-4c0b-bfcc-4d25da702dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torchvision, torchvision.transforms as transforms\n",
        "import torch, torch.nn as nn\n",
        "import random, subprocess, os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "# To read and write to google drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDuKFrqzSdkl",
        "colab_type": "text"
      },
      "source": [
        "# Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v16VY8ymSz54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_loader_stl(crop_size: int = 33, batch_size: int = 128, num_workers: int = 1, scale: float = 2.0):\n",
        "    \"\"\"\n",
        "    Loads the dataloader of the STL-10 Dataset using the given specifications with the required \n",
        "                          augmentation schemes\n",
        "    input : crop_size -> image size of the square sub images the model has been trained on\n",
        "            scale     -> Scale by which the low resolution image is downscaled  \n",
        "    output: dataloader iterable to be able to train on the images\n",
        "\n",
        "    Augmentation Schemes: Since torch has strong built in support for transforms, augmentation\n",
        "                          was done within our dataloader transforms employing TenCrop on each \n",
        "                          image. For every image we get 5 crops (Center + 4 corners) and the horizontal \n",
        "                          flip of each. TenCrop returns a tuple, which was handled using lambda \n",
        "                          and also in the training script in the cell below.\n",
        "          \n",
        "    \"\"\"\n",
        "    # Write transforms for TenCrop and for generating low res images using bicubic interpolation (interpolation = 3)\n",
        "    transform_high_res = transforms.Compose([\n",
        "            transforms.TenCrop(crop_size),\n",
        "            transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))\n",
        "        ])\n",
        "    transform_low_res = transforms.Compose([\n",
        "            transforms.Resize(int(96 / scale), interpolation=3),\n",
        "            transforms.Resize(96, interpolation=3),\n",
        "            transform_high_res\n",
        "        ])\n",
        "    \n",
        "    # Make STL-10 dataset object\n",
        "    dataset_high_res = torchvision.datasets.STL10('.', transform = transform_high_res, download = True)\n",
        "    dataset_low_res = torchvision.datasets.STL10('.', transform = transform_low_res, download = False)\n",
        "\n",
        "    # Create the dataloader object using the transforms (Not shuffled since we will be checking progress on the same examples)\n",
        "    dataloader_high_res = torch.utils.data.DataLoader(dataset_high_res, batch_size = batch_size, num_workers = num_workers, shuffle = False)\n",
        "    dataloader_low_res = torch.utils.data.DataLoader(dataset_low_res, batch_size = batch_size, num_workers = num_workers, shuffle = False)\n",
        "    return dataloader_low_res, dataloader_high_res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQi_104rSg7E",
        "colab_type": "text"
      },
      "source": [
        "# Models used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOXcyN7bR5tJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SuperResolution(nn.Module):\n",
        "    \"\"\"\n",
        "    Network Architecture as per specified in the paper. \n",
        "    The chosen configuration for successive filter sizes are 9-5-5\n",
        "    The chosed configuration for successive filter depth are 128-64(-3)\n",
        "    \"\"\"\n",
        "    def __init__(self, sub_image: int = 33, spatial: list = [9, 5, 5], filter: list = [128, 64], num_channels: int = 3):\n",
        "        super().__init__()\n",
        "        self.layer_1 = nn.Conv2d(num_channels, filter[0], spatial[0], padding = spatial[0] // 2)\n",
        "        self.layer_2 = nn.Conv2d(filter[0], filter[1], spatial[1], padding = spatial[1] // 2)\n",
        "        self.layer_3 = nn.Conv2d(filter[1], num_channels, spatial[2], padding = spatial[2] // 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, image_batch):\n",
        "        x = self.layer_1(image_batch)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer_2(x)\n",
        "        y = self.relu(x)\n",
        "        x = self.layer_3(y)\n",
        "        return x, y "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYT-GdjCSj-k",
        "colab_type": "text"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLzng-oCSEGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    \"\"\"\n",
        "    Train function for training and constantly visualizing intermediate layers and \n",
        "    immediate outputs. All images relevant, along with losses are tracked on tensorboard\n",
        "    in the first cell of this notebook. All hyperparameters are directly embedded in the\n",
        "    code since the model has few to begin with, and the ones that exist also have fairly\n",
        "    standard values\n",
        "\n",
        "    We achieve lesser PSNR with the same configurations as the paper since we train for \n",
        "    much lesser steps (They train for 10^8 backward steps), since complete training \n",
        "    according to the paper was simply infeasible given the idle time of a colab notebook \n",
        "    is only 90 minutes \n",
        "    \"\"\"\n",
        "    # Initialize model, data, writer, optimizer, and backward count\n",
        "    low_res_loader, high_res_loader = load_loader_stl()\n",
        "    model = SuperResolution()\n",
        "\n",
        "    # Comment the below line out, if training from scratch - Tensorboard graphs make more\n",
        "    #                                      sense if training trends are seen from scratch\n",
        "    #model.load_state_dict(torch.load('/content/drive/My Drive/isr/isr_best.pth')) \n",
        "    # ----------------------------------------------> To continue training if desired\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), 1e-04)\n",
        "    writer = SummaryWriter()\n",
        "    n = 0\n",
        "\n",
        "    for epoch in tqdm(range(500), desc= \"Training\", ncols = 120):\n",
        "        for low_res, high_res in zip(low_res_loader, high_res_loader):\n",
        "\n",
        "            # Convert TenCrop tuple into a trainable shape of (batch_size * 10, c, h, w)\n",
        "            low_res_batch, high_res_batch = low_res[0], high_res[0]\n",
        "            _, _, c, h, w = low_res_batch.size()\n",
        "            low_res_batch, high_res_batch = low_res_batch.to(device), high_res_batch.to(device)\n",
        "            low_res_batch, high_res_batch = low_res_batch.view(-1, c, h, w), high_res_batch.view(-1, c, h, w)\n",
        "            reconstructed_batch, intermediate = model(low_res_batch)\n",
        "\n",
        "            # Calculate gradients and make a backward step on MSE loss\n",
        "            loss_fn = nn.MSELoss()\n",
        "            loss = loss_fn(high_res_batch, reconstructed_batch)\n",
        "            loss_to_compare = loss_fn(high_res_batch, low_res_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Clamp the image between 0 and 1 and prepare transforms and image arrays to write on tensorboard  \n",
        "            to_pil = torchvision.transforms.ToPILImage()\n",
        "            resize = torchvision.transforms.Resize((48 * 7, 144 * 7))\n",
        "            other_resize = torchvision.transforms.Resize((48 * 5, 48 * 5))\n",
        "            to_tensor = torchvision.transforms.ToTensor()\n",
        "            ind = 4\n",
        "            image = to_pil(torch.cat((low_res_batch[ind], high_res_batch[ind], reconstructed_batch[ind]), dim = 2).cpu())\n",
        "            image = to_tensor(resize(image))\n",
        "            image = image.clamp(0, 1)\n",
        "            n += 1\n",
        "            psnr = 10 * torch.log10(1 / loss)\n",
        "            psnr_tc = 10 * torch.log10(1 / loss_to_compare)\n",
        "\n",
        "            # Write relevant scalars and comparitive images on tensorboard\n",
        "            writer.add_scalar(\"MSE loss\", loss * (255 ** 2), n)\n",
        "            writer.add_scalar(\"PSNR of Reconstruction\", psnr, n)\n",
        "            writer.add_scalar(\"PSNR of BiCubic Interpolation (For comparision)\", psnr_tc, n)\n",
        "            writer.add_image(\"Low Resolution Image | High Resolution Image | Reconstructed Image\", image, n, dataformats='CHW')\n",
        "\n",
        "        # Choose image on who intermediate layers are to be visualized and the channels to visualize\n",
        "        index = 30 #Chooses image patch to visualize on, up till 80 (Size of the remnant batch)\n",
        "        channels_to_visualize = [1, 2, 3, 4, 5, 6, 7, 8]  #Channel numbers out of 64 to visualize \n",
        "\n",
        "        # Write the intermediate layer visualizations and also write to drive, to download and create animated gifs\n",
        "        patch = to_tensor(other_resize(to_pil(high_res_batch.detach().cpu()[index])))\n",
        "        writer.add_image(\"Image Patch {}\".format(index), patch, n, dataformats='CHW')\n",
        "        pil_patch = to_pil(patch)\n",
        "        pil_patch.save('/content/drive/My Drive/isr/patch_{}.png'.format(index))\n",
        "\n",
        "        # Write the progress of training on two standard examples - 25 and 30 0f last batch\n",
        "        os.makedirs('/content/drive/My Drive/isr/r1', exist_ok=True)\n",
        "        r1 = to_tensor(other_resize(to_pil(reconstructed_batch.detach().cpu()[25])))\n",
        "        pil_r1 = to_pil(r1)\n",
        "        pil_r1.save('/content/drive/My Drive/isr/r1/frame_{}.png'.format(epoch))\n",
        "\n",
        "        os.makedirs('/content/drive/My Drive/isr/r2', exist_ok=True)\n",
        "        r2 = to_tensor(other_resize(to_pil(reconstructed_batch.detach().cpu()[30])))\n",
        "        pil_r2 = to_pil(r2)\n",
        "        pil_r2.save('/content/drive/My Drive/isr/r2/frame_{}.png'.format(epoch))\n",
        "\n",
        "        for feature in channels_to_visualize:\n",
        "            os.makedirs('/content/drive/My Drive/isr/channel_{}'.format(feature), exist_ok=True)\n",
        "            visualization = to_tensor(other_resize(to_pil(intermediate.detach().cpu()[index, feature,:,:])))\n",
        "            writer.add_image(\"Channel {}\".format(feature), visualization, n, dataformats='CHW')\n",
        "            pil_vis = to_pil(visualization)\n",
        "            pil_vis.save('/content/drive/My Drive/isr/channel_{}/frame_{}.png'.format(feature, epoch))\n",
        "            \n",
        "        # Save the model for every epoch\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/isr/isr_best_2.pth'.format(n)) \n",
        "    \n",
        "    return model\n",
        "\n",
        "model = train()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}